---
layout: post
title: Multi-task Learning
date: 2024-01-13 18:43:00-0400
tags: deep-learning
categories: notes
giscus_comments: true
related_posts: false
---

- The tasks need to have a shared structure. If the tasks are entirely different from each other, then Meta Learning does not work well. However, in practice, a lot of tasks do have some common shared strucutre -- the laws of physics, rules of language remain the same etc. 
- Key Assumption: Meta-training and Meta-learning tasks come from the same distribution
- A task is defined as $$\{p_i(x), p_i(y|x), \mathcal{L}_i\}$$ where $$\mathcal{L}_i$$ is the loss function. In practice, we don't have access to $$p_i(x)$$ and $$p_i(y|x)$$, we only have access to the dataset generated by these. 
	- Multi-task classification: $$\mathcal{L}_i$$'s are the same. e.g: per-language handwriting recongition, personalized spam filtering. 
	- Multi-label learning: $$p_i(x)$$ and $$\mathcal{L}_i$$ are same across the task. e.g: face attribute recongition such as black/brown hair, blue/brown eyes. Here all the images are the same. Scene understanding - predicting depth, key points and the surface normal. Here too, all the images in the dataset are same.   
	- Loss function can vary as well, e.g: some labels might be discrete and some might be continuous.
- Task descriptor $$\Large\textbf z_i$$ - we need to tell the neural network what the current task is. 
	- one-hot encoding or whatever metadata we have about the task
		- having a rich representation for task is better than one-hot encoding because as the one-hot encodings are orthogonal to each other, we are not giving any information to the network about the shared structure among the tasks.
- Conditioning the network on the task $$f_\theta(y | x, z_i)$$
	- Split parameters into shared parameters and task specific parameters $$\theta^{sh}$$ and $$\theta^i$$ 
		- Choosing how to condition on $$z_i$$ is equivalent to choosing how and where to split the parameters.
	- Use same parameters for all task, and add/concatenate $$z_i$$ to the output of some intermediate layer. 
	- Multi-head architecture:
		- shared bottom layers and task-specific layers
	- Multiplicative conditioning: 
		- map $$z_i$$ to a condition representation and take dot product with input to some layer. 
		- is more expressive and generally works better. 
		- we could learn the mapping (from one-hot to dense embedding) from the data end-to-end, but it is better if we could develop something like "word vectors" but for tasks. 
- Objective function:
	- Vanilla MTL objective - sum up the loss over all the tasks.
		- Sample mini-batches of tasks
		- Sample mini-batch data points for each task 
		- Compute loss on the mini-batch 
		- Backpropogate to compute gradients and use optimizer to update the weights. 
	- Weight different tasks differently
		- manually choose based on importance of task
		- heuristics e.g: encourage gradients to have same magnitude
		- optimize for the worst case task loss - find the worst loss task and minimize just that. 
			- min-max objective, harder optimization problem. 
			- usefull in fairness settings. 
			- will end up in same loss values for all tasks
- Challenges:
	- Negative transfer: multi-task learning performs worse than training independentally. Because of
		- Optimization challenges: tasks may be learning at different rates, cross-talk interference. 
		- Limited representation capacity - networks need to be larger than single task networks as they are trying to learn more.  
		- In case of negative transfer, share less parameters across tasks. 
	- What to do if we have lot of tasks - which of them are similar and which are complementary
		- Open problem: no closed-form solution to measuring task similarity. 
		- There are ways to approximate task similarity from single training run.