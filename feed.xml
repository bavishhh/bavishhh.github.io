<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://bavishhh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bavishhh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-11T16:02:32+00:00</updated><id>https://bavishhh.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Hyperbolic Deep Reinforcement Learning</title><link href="https://bavishhh.github.io/blog/2024/hyperbolic-deep-rl/" rel="alternate" type="text/html" title="Hyperbolic Deep Reinforcement Learning"/><published>2024-12-31T17:24:00+00:00</published><updated>2024-12-31T17:24:00+00:00</updated><id>https://bavishhh.github.io/blog/2024/hyperbolic-deep-rl</id><content type="html" xml:base="https://bavishhh.github.io/blog/2024/hyperbolic-deep-rl/"><![CDATA[<blockquote> <p>Notes of Paper Hyperbolic Deep Reinforcement Learning by Cetin <em>et al.</em>, 2023. <a href="https://openreview.net/pdf?id=TfBHFLgv77">Link</a></p> </blockquote> <h2 id="contributions">Contributions</h2> <ul> <li>Show (experimentally) learning hierarchical features improve generalization</li> <li>Regularization approach to stabilize the training of hyperbolic neural network for RL</li> </ul> <h2 id="notes">Notes</h2> <h3 id="prelims">Prelims</h3> <ul> <li>Hyperbolic ML <ul> <li>The geodesic (shortest path) between two points on the Poincare ball model is the arc perpendicular to the boundary.</li> <li>Hyperbolic spaces — continuous analog of trees</li> <li>The volume of the ball grows exponentially with the radius</li> </ul> </li> </ul> <h3 id="method">Method</h3> <ul> <li>Definition: \(\delta\)-hyperbolicity <ul> <li>Geodesic triangle — for any three points A, B, and C in the space, consider the triangle formed by connecting the points pairwise using the shortest line segment (in that space).</li> <li>For every point on a line segment, if there exists a point on the other line segment within \(\delta\) distance, then the triangle is called \(\delta\)-slim.</li> <li>A metric space is called \(\delta\)-hyperbolic if every geodesic triangle ABC is \(\delta\)-slim.</li> <li>For a tree, any point simultaneously lies on two line segments, so \(\delta = 0\).</li> <li>\(\delta\)-hyperbolicity can be interpreted as a measure of deviation from the tree structure.</li> </ul> </li> <li>The learned encodings of the states span finite subset of eucledian space — a discrete metric space</li> <li>Hyperbolicity of RL <ul> <li>State evolution in a trajectory can be conceptualized as a tree due to the Markovian property.</li> <li>Non-hierarchical information of the state (such as general appearance) should be ignored, otherwise the policy and value networks overfit to spurious correlations</li> <li>Train the PPO agent with IMPALA architecture and analyze how performance and \(\delta\)-hyperbolicity measure evolove during the training. <ul> <li>Compute the \(\delta\)-hyperbolicity for the learned representations</li> <li>Normalize using the diameter so that the values lie between 0 and 1.</li> <li>If it is 0, then perferct tree like structure; if 1, then perfect non-hyperbolic space..</li> </ul> </li> </ul> </li> <li>Training with hyperbolic latent spaces <ul> <li>Replacing the final relu and linear layer with exponential map and gyroplane fully connected layer led to underwhelming performance. <ul> <li>Hyperbolic policy struggles to start exploring and becoming more deterministic with policy improvement as expected from PPO’s entropy bonus. <ul> <li>This indicates optimization challenges of end-to-end training of RL with hyperbolic representations.</li> <li>Using techniques from prior work — careful initialization, representaiton clipping did not help. <ul> <li>These techniques facilitate learning of approriate angular layouts initially. But in RL, the non-stationarity makes the early angular layouts suboptimal in the long run.</li> </ul> </li> <li>S-RYM — Spectrally Regularized Hyperbolic Mappings <ul> <li>The paper takes inspiration from GAN literature, where there is non-stationarity as well, to make use of spectral normalization, which has been shown to prevent exploding gradients phenomenon.</li> <li>Spectral normalization is applied to all eucledian layers expect the final hyperbolic part.</li> <li>final latent representation is scaled before mapping to \(\mathbb{B}^n\) so that modifying the dimensionality of representations should not significantly affect their own and gradient’s magnitudes</li> <li>this resloves the optimization challenges, achieves high performance compared to the eucledian implementation, and maintains low gradient norm.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="observations">Observations</h3> <ul> <li>\(\delta\) -hyperbolicity <ul> <li>\(\delta\) drops to low values in the inital training period for all environements</li> <li>In fruitbot and starpilot, \(\delta\) keeps further decreasing after the intial drop <ul> <li>In these environments, the generalization gap is low</li> </ul> </li> <li>In bigfish and dogeball, \(\delta\) starts increasing slowly after the initial drop, indicating the features start losing their hierarchical strucutre. <ul> <li>In these enviroments, the agent starts overfitting and the test performance is low.</li> </ul> </li> </ul> </li> <li>S-RYM <ul> <li>Teseted on ProcGen benchmark</li> <li>Tried with PPO and Rainbow DQN</li> <li>Tried random crop data augmentation (which is used to improve generalization) but was outperformed by hyperbolic implementation.</li> <li>Reduced the dimensionality of final representation from 256 to 32, which provided further improvement in performance.</li> <li>Tested on Atari, observed improvement over eucledian implementation.</li> </ul> </li> </ul> <h3 id="limitations">Limitations</h3> <ul> <li>Spectral normalization limits the expressivity of the eucledian part of the network.</li> <li>The training time is higher <ul> <li>Most of the slowdown is due to the power iteration used in spectral normalization.</li> </ul> </li> <li>Fixed curvature might not always yield the appropriate inductive bias</li> </ul> <h2 id="references">References</h2> <p><a href="https://towardsdatascience.com/hyperbolic-deep-reinforcement-learning-b2de787cf2f7">https://towardsdatascience.com/hyperbolic-deep-reinforcement-learning-b2de787cf2f7</a></p> <p><a href="https://www.reddit.com/r/MachineLearning/comments/xzfmk8/r_hyperbolic_deep_reinforcement_learning_they/">https://www.reddit.com/r/MachineLearning/comments/xzfmk8/r_hyperbolic_deep_reinforcement_learning_they/</a></p> <p><a href="https://bjlkeng.io/posts/hyperbolic-geometry-and-poincare-embeddings/">https://bjlkeng.io/posts/hyperbolic-geometry-and-poincare-embeddings/</a></p> <h2 id="implementations">Implementations</h2> <p><a href="https://github.com/twitter-research/hyperbolic-rl">https://github.com/twitter-research/hyperbolic-rl</a></p>]]></content><author><name></name></author><category term="notes"/><category term="deep-learning"/><summary type="html"><![CDATA[Notes of Paper Hyperbolic Deep Reinforcement Learning by Cetin et al., 2023. Link]]></summary></entry><entry><title type="html">Multi-task Learning</title><link href="https://bavishhh.github.io/blog/2024/multi-task-learning/" rel="alternate" type="text/html" title="Multi-task Learning"/><published>2024-01-13T22:43:00+00:00</published><updated>2024-01-13T22:43:00+00:00</updated><id>https://bavishhh.github.io/blog/2024/multi-task-learning</id><content type="html" xml:base="https://bavishhh.github.io/blog/2024/multi-task-learning/"><![CDATA[<ul> <li>The tasks need to have a shared structure. If the tasks are entirely different from each other, then Meta Learning does not work well. However, in practice, a lot of tasks do have some common shared strucutre – the laws of physics, rules of language remain the same etc.</li> <li>Key Assumption: Meta-training and Meta-learning tasks come from the same distribution.</li> <li>A task is defined as \(\{ p_i(x), p_i(y \vert x), \mathcal{L}_i \}\) where \(\mathcal{L}_i\) is the loss function. In practice, we don’t have access to \(p_i(x)\) and \(p_i(y \vert x)\), we only have access to the dataset generated by these. <ul> <li>Multi-task classification: \(\mathcal{L}_i\)’s are the same. e.g: per-language handwriting recongition, personalized spam filtering.</li> <li>Multi-label learning: \(p_i(x)\) and \(\mathcal{L}_i\) are same across the task. e.g: face attribute recongition such as black/brown hair, blue/brown eyes. Here all the images are the same. Scene understanding - predicting depth, key points and the surface normal. Here too, all the images in the dataset are same.</li> <li>Loss function can vary as well, e.g: some labels might be discrete and some might be continuous.</li> </ul> </li> <li>Task descriptor \(z_i\) - we need to tell the neural network what the current task is. <ul> <li>one-hot encoding or whatever metadata we have about the task <ul> <li>having a rich representation for task is better than one-hot encoding because as the one-hot encodings are orthogonal to each other, we are not giving any information to the network about the shared structure among the tasks.</li> </ul> </li> </ul> </li> <li>Conditioning the network on the task \(f_\theta(y \vert x, z_i)\) <ul> <li>Split parameters into shared parameters and task specific parameters \(\theta^{sh}\) and \(\theta^i\) <ul> <li>Choosing how to condition on \(z_i\) is equivalent to choosing how and where to split the parameters.</li> </ul> </li> <li>Use same parameters for all task, and add/concatenate \(z_i\) to the output of some intermediate layer.</li> <li>Multi-head architecture: <ul> <li>shared bottom layers and task-specific layers</li> </ul> </li> <li>Multiplicative conditioning: <ul> <li>map \(z_i\) to a condition representation and take dot product with input to some layer.</li> <li>is more expressive and generally works better.</li> <li>we could learn the mapping (from one-hot to dense embedding) from the data end-to-end, but it is better if we could develop something like “word vectors” but for tasks.</li> </ul> </li> </ul> </li> <li>Objective function: <ul> <li>Vanilla MTL objective - sum up the loss over all the tasks. <ul> <li>Sample mini-batches of tasks</li> <li>Sample mini-batch data points for each task</li> <li>Compute loss on the mini-batch</li> <li>Backpropogate to compute gradients and use optimizer to update the weights.</li> </ul> </li> <li>Weight different tasks differently <ul> <li>manually choose based on importance of task</li> <li>heuristics e.g: encourage gradients to have same magnitude</li> <li>optimize for the worst case task loss - find the worst loss task and minimize just that. <ul> <li>min-max objective, harder optimization problem.</li> <li>usefull in fairness settings.</li> <li>will end up in same loss values for all tasks</li> </ul> </li> </ul> </li> </ul> </li> <li>Challenges: <ul> <li>Negative transfer: multi-task learning performs worse than training independentally. Because of <ul> <li>Optimization challenges: tasks may be learning at different rates, cross-talk interference.</li> <li>Limited representation capacity - networks need to be larger than single task networks as they are trying to learn more.</li> <li>In case of negative transfer, share less parameters across tasks.</li> </ul> </li> <li>What to do if we have lot of tasks - which of them are similar and which are complementary <ul> <li>Open problem: no closed-form solution to measuring task similarity.</li> <li>There are ways to approximate task similarity from single training run.</li> </ul> </li> </ul> </li> </ul>]]></content><author><name></name></author><category term="notes"/><category term="deep-learning"/><summary type="html"><![CDATA[The tasks need to have a shared structure. If the tasks are entirely different from each other, then Meta Learning does not work well. However, in practice, a lot of tasks do have some common shared strucutre – the laws of physics, rules of language remain the same etc. Key Assumption: Meta-training and Meta-learning tasks come from the same distribution. A task is defined as \(\{ p_i(x), p_i(y \vert x), \mathcal{L}_i \}\) where \(\mathcal{L}_i\) is the loss function. In practice, we don’t have access to \(p_i(x)\) and \(p_i(y \vert x)\), we only have access to the dataset generated by these. Multi-task classification: \(\mathcal{L}_i\)’s are the same. e.g: per-language handwriting recongition, personalized spam filtering. Multi-label learning: \(p_i(x)\) and \(\mathcal{L}_i\) are same across the task. e.g: face attribute recongition such as black/brown hair, blue/brown eyes. Here all the images are the same. Scene understanding - predicting depth, key points and the surface normal. Here too, all the images in the dataset are same. Loss function can vary as well, e.g: some labels might be discrete and some might be continuous. Task descriptor \(z_i\) - we need to tell the neural network what the current task is. one-hot encoding or whatever metadata we have about the task having a rich representation for task is better than one-hot encoding because as the one-hot encodings are orthogonal to each other, we are not giving any information to the network about the shared structure among the tasks. Conditioning the network on the task \(f_\theta(y \vert x, z_i)\) Split parameters into shared parameters and task specific parameters \(\theta^{sh}\) and \(\theta^i\) Choosing how to condition on \(z_i\) is equivalent to choosing how and where to split the parameters. Use same parameters for all task, and add/concatenate \(z_i\) to the output of some intermediate layer. Multi-head architecture: shared bottom layers and task-specific layers Multiplicative conditioning: map \(z_i\) to a condition representation and take dot product with input to some layer. is more expressive and generally works better. we could learn the mapping (from one-hot to dense embedding) from the data end-to-end, but it is better if we could develop something like “word vectors” but for tasks. Objective function: Vanilla MTL objective - sum up the loss over all the tasks. Sample mini-batches of tasks Sample mini-batch data points for each task Compute loss on the mini-batch Backpropogate to compute gradients and use optimizer to update the weights. Weight different tasks differently manually choose based on importance of task heuristics e.g: encourage gradients to have same magnitude optimize for the worst case task loss - find the worst loss task and minimize just that. min-max objective, harder optimization problem. usefull in fairness settings. will end up in same loss values for all tasks Challenges: Negative transfer: multi-task learning performs worse than training independentally. Because of Optimization challenges: tasks may be learning at different rates, cross-talk interference. Limited representation capacity - networks need to be larger than single task networks as they are trying to learn more. In case of negative transfer, share less parameters across tasks. What to do if we have lot of tasks - which of them are similar and which are complementary Open problem: no closed-form solution to measuring task similarity. There are ways to approximate task similarity from single training run.]]></summary></entry><entry><title type="html">Meta Learning</title><link href="https://bavishhh.github.io/blog/2024/meta-learning/" rel="alternate" type="text/html" title="Meta Learning"/><published>2024-01-07T03:17:00+00:00</published><updated>2024-01-07T03:17:00+00:00</updated><id>https://bavishhh.github.io/blog/2024/meta-learning</id><content type="html" xml:base="https://bavishhh.github.io/blog/2024/meta-learning/"><![CDATA[<blockquote> <p><strong>Goal:</strong> Given data from many tasks \(\mathcal{T}_1, ..., \mathcal{T_n}\) solve new task \(\mathcal{T}_{new}\) more quickly/proficiently/stably</p> </blockquote> <ul> <li>Can we explicitly optimize for transferability? <ul> <li>In Transfer Learning we start from a model that has been already trained for some task and hope that it helps the target task.</li> <li>In Multi-Task Learning we solve all the tasks together</li> </ul> </li> <li><strong>Key Assumption</strong>: meta-training tasks and meta-test task drawn i.i.d from same task distribution</li> <li>Tasks must share structure</li> <li>Two optimizers – learner, which learns new tasks, and meta-learner, which trains the learner.</li> </ul> <table> <thead> <tr> <th style="text-align: left">Mechanistic View</th> <th style="text-align: left">Probabilistic View</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Deep network that can read in an entire dataset and make predictions for new datapoints</td> <td style="text-align: left">Extract shared prior knowledge from a set of tasks that allows efficient learning of new tasks</td> </tr> <tr> <td style="text-align: left">Training this network uses a meta-dataset, which itself consists of many datasets, each for a different task</td> <td style="text-align: left">Learning a new task uses this prior and a small training set to infer most likely posterior parameters</td> </tr> </tbody> </table> <ul> <li>Bayesian view: <ul> <li>Shared structure between tasks is task parameters \(\phi_i\) become independent when conditioned on \(\theta\) (\(\phi_i \perp\!\!\!\perp \phi_j \vert \theta\)), and are otherwise not independent \(\implies\) lower entropy distribution while conditioned on theta compared to prior over \(\phi_i\)</li> <li>Therefore, once we identify \(\theta\) learning \(\phi\) should be faster since we have fewer bits to uncover from training data points</li> </ul> </li> </ul> <h3 id="meta-supervised-learning">Meta Supervised Learning</h3> <ul> <li>Input: dataset \(\mathcal{D}^{tr}\) and test data point \(x^{ts}\), where \(\mathcal{D}^{tr} : \{(x, y)_{1:K}\}\) (K-shot learning)</li> <li>Output: label \(y^{ts}\)</li> <li>Data: \(\{\mathcal{D}_i\}\) where \(\mathcal{D}_i : \{(x, y)_j\}\)</li> <li>Reduces the meta-learning problem to the design and optimization of \(f\)</li> </ul> \[\large{y^{ts} = f_\theta(\mathcal{D}^{tr}, x^{ts})}\] <h3 id="black-box-adaptation">Black-Box Adaptation</h3> <ul> <li><strong>Key Idea:</strong> <ul> <li>Train a neural network to represent \(\phi_i = f_\theta(\mathcal{D}^{tr}_i)\) (learner)</li> <li>Predict test poins using neural network parameterized by \(\phi_i\)</li> </ul> </li> <li><strong>Algorithm</strong> <ol> <li>Sample task \(\mathcal{T}_i\) (or mini batch of tasks)</li> <li>Sample disjoint datasets \(\mathcal{D}^{tr}_i, \mathcal{D}^{test}_i\) from \(\mathcal{D}_i\)</li> <li>Compute \(\phi_i = f_\theta(\mathcal{D}^{tr}_i)\)</li> <li>Update \(\theta\) using \(\nabla_\theta \mathcal{L}(\phi_i, \mathcal{D}^{test}_i)\)</li> <li>Repeat</li> </ol> </li> <li><strong>Challenge</strong>: outputting all the neural net parameters does not seem scalable <ul> <li>Do not need to output all parameters, only sufficient statistic</li> <li>low-dimensional vector \(h_i\) - represents contextual task information</li> <li> \[\phi_i = \{h_i, \theta_g\}\] </li> </ul> </li> </ul> <h3 id="optimization-based-approach">Optimization based approach</h3> <ul> <li><strong>Algorithm</strong> (MAML) <ol> <li>Randomly initialize \(\theta\)</li> <li>Sample task (or mini batch of tasks)</li> <li>Sample disjoint datasets \(\mathcal{D}^{tr}_i, \mathcal{D}^{test}_i\) from \(\mathcal{D}_i\)</li> <li>Using \(\theta\) as the initial parameters, fine tune on \(\mathcal{D}^{tr}\) to obtain \(\phi\)</li> </ol> \[\phi_i = \theta - \alpha \nabla_\theta \mathcal{L}(\theta, \mathcal{D}_i^{tr})\] <ol> <li>Update \(\theta\) based on the loss \(\nabla_\theta \mathcal{L}(\phi_i, \mathcal{D}^{test}_i)\)</li> <li>Repeat from step 2</li> </ol> </li> <li><strong>Intuition</strong> <ul> <li>Find \(\theta\) such that it is very close to \(\phi_i^*\) i.e., optimal parameters of all the tasks.</li> </ul> </li> <li>Need to compute second order derivatives - Hessian <ul> <li>Computing Hessian is expensive - \(O(n^2)\)</li> <li>Can use hessian vector product instead</li> </ul> </li> </ul> <h1 id="further-reading">Further Reading</h1> <ul> <li>Learning to Learn by Chelsea Finn <a href="https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/">blog</a></li> </ul>]]></content><author><name></name></author><category term="notes"/><category term="deep-learning"/><summary type="html"><![CDATA[Goal: Given data from many tasks \(\mathcal{T}_1, ..., \mathcal{T_n}\) solve new task \(\mathcal{T}_{new}\) more quickly/proficiently/stably]]></summary></entry><entry><title type="html">Graph Laplacian</title><link href="https://bavishhh.github.io/blog/2023/graph-laplacian/" rel="alternate" type="text/html" title="Graph Laplacian"/><published>2023-12-08T23:26:00+00:00</published><updated>2023-12-08T23:26:00+00:00</updated><id>https://bavishhh.github.io/blog/2023/graph-laplacian</id><content type="html" xml:base="https://bavishhh.github.io/blog/2023/graph-laplacian/"><![CDATA[<ul> <li>In eucledian space, Laplacian is defined as divergence of the gradient of the function</li> </ul> \[L = \nabla\cdot \nabla f\] <ul> <li>For graph laplacian, we need to ask: <ul> <li>What is the meaning of function applied to a graph?</li> <li>What is the meaning of gradient of a graph function?</li> <li>What is the divergence of the graph gradient?</li> </ul> </li> <li>A graph function assings a number to each node. Eg: The number of friends a person has in a social network graph.</li> <li>In eucledian space, gradient is the change in the function in each direction. Extending this to graphs, the gradient is the array of difference of function values along each edge.</li> </ul> \[\nabla f = K^Tf\] <p>where K is the incidence matrix, i.e., for every edge (u, v), K[u, e]= +1 and K[v, e] = -1</p> <ul> <li>In eucledian space, divergence in the net outward flux of the vector field at a point. In graphs, the vector field is the gradient, and divergence of gradient g will be</li> </ul> \[\nabla g = Kg \\ = KK^Tf\] <p>Therefore, \(L = KK^T = D - W\) where D is the degree matrix and A is the adjacency matrix.</p> <ul> <li>The matrix \(KK^T\) has diagonal entries equal to the degree of the vertex and the {ij}-th entry is the number of edges connecting the vertices i and j.</li> <li>Laplacian is the second derivative, so it tells how smooth the function is. Similarly, the laplacian of a graph function tells us how smooth the graph function is.</li> <li>In the case of graphs, a smooth function is the one which doesn’t change abruptly when you traverse along the edges, but it can jump abruptly along nodes that are not connected.</li> <li>The gradient maps a function on nodes to function on edges, and the divergence maps it back to function on nodes (\(KK^T\))</li> </ul> <p><strong>References:</strong></p> <ul> <li><a href="https://www.quora.com/Whats-the-intuition-behind-a-Laplacian-matrix-Im-not-so-much-interested-in-mathematical-details-or-technical-applications-Im-trying-to-grasp-what-a-laplacian-matrix-actually-represents-and-what-aspects-of-a-graph-it-makes-accessible/answer/Muni-Sreenivas-Pydi">https://www.quora.com/Whats-the-intuition-behind-a-Laplacian-matrix-Im-not-so-much-interested-in-mathematical-details-or-technical-applications-Im-trying-to-grasp-what-a-laplacian-matrix-actually-represents-and-what-aspects-of-a-graph-it-makes-accessible/answer/Muni-Sreenivas-Pydi</a></li> <li><a href="https://www.quora.com/Whats-the-intuition-behind-a-Laplacian-matrix-Im-not-so-much-interested-in-mathematical-details-or-technical-applications-Im-trying-to-grasp-what-a-laplacian-matrix-actually-represents-and-what-aspects-of-a-graph-it-makes-accessible/answer/Alex-Kritchevsky">https://www.quora.com/Whats-the-intuition-behind-a-Laplacian-matrix-Im-not-so-much-interested-in-mathematical-details-or-technical-applications-Im-trying-to-grasp-what-a-laplacian-matrix-actually-represents-and-what-aspects-of-a-graph-it-makes-accessible/answer/Alex-Kritchevsky</a></li> <li><a href="https://mathoverflow.net/questions/368963/intuitively-what-does-a-graph-laplacian-represent">https://mathoverflow.net/questions/368963/intuitively-what-does-a-graph-laplacian-represent</a></li> <li><a href="https://mbernste.github.io/posts/laplacian_matrix/">https://mbernste.github.io/posts/laplacian_matrix/</a></li> <li><a href="http://math.uchicago.edu/~may/REU2022/REUPapers/Li,Hanchen.pdf">http://math.uchicago.edu/~may/REU2022/REUPapers/Li,Hanchen.pdf</a></li> </ul>]]></content><author><name></name></author><category term="notes"/><category term="graphs"/><summary type="html"><![CDATA[In eucledian space, Laplacian is defined as divergence of the gradient of the function]]></summary></entry></feed>